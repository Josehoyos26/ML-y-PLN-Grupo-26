{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 - Clasificación de género de películas\n",
    "\n",
    "El propósito de este proyecto es que puedan poner en práctica, en sus respectivos grupos de trabajo, sus conocimientos sobre técnicas de preprocesamiento, modelos predictivos de NLP, y la disponibilización de modelos. Para su desarrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 2: Clasificación de género de películas\"\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 8. Sin embargo, es importante que avancen en la semana 7 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 8, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/t/2c54d005f76747fe83f77fbf8b3ec232)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos para la predicción de género en películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/moviegenre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se usará un conjunto de datos de géneros de películas. Cada observación contiene el título de una película, su año de lanzamiento, la sinopsis o plot de la película (resumen de la trama) y los géneros a los que pertenece (una película puede pertenercer a más de un género). Por ejemplo:\n",
    "- Título: 'How to Be a Serial Killer'\n",
    "- Plot: 'A serial killer decides to teach the secrets of his satisfying career to a video store clerk.'\n",
    "- Generos: 'Comedy', 'Crime', 'Horror'\n",
    "\n",
    "La idea es que usen estos datos para predecir la probabilidad de que una película pertenezca, dada la sinopsis, a cada uno de los géneros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agradecemos al profesor Fabio González, Ph.D. y a su alumno John Arevalo por proporcionar este conjunto de datos. Ver https://arxiv.org/abs/1702.01992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo predicción conjunto de test para envío a Kaggle\n",
    "En esta sección encontrarán el formato en el que deben guardar los resultados de la predicción para que puedan subirlos a la competencia en Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2003</td>\n",
       "      <td>Most</td>\n",
       "      <td>most is the story of a single father who takes...</td>\n",
       "      <td>['Short', 'Drama']</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2008</td>\n",
       "      <td>How to Be a Serial Killer</td>\n",
       "      <td>a serial killer decides to teach the secrets o...</td>\n",
       "      <td>['Comedy', 'Crime', 'Horror']</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>1941</td>\n",
       "      <td>A Woman's Face</td>\n",
       "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
       "      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1954</td>\n",
       "      <td>Executive Suite</td>\n",
       "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>1990</td>\n",
       "      <td>Narrow Margin</td>\n",
       "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
       "      <td>['Action', 'Crime', 'Thriller']</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      title  \\\n",
       "3107  2003                       Most   \n",
       "900   2008  How to Be a Serial Killer   \n",
       "6724  1941             A Woman's Face   \n",
       "4704  1954            Executive Suite   \n",
       "2582  1990              Narrow Margin   \n",
       "\n",
       "                                                   plot  \\\n",
       "3107  most is the story of a single father who takes...   \n",
       "900   a serial killer decides to teach the secrets o...   \n",
       "6724  in sweden ,  a female blackmailer with a disfi...   \n",
       "4704  in a friday afternoon in new york ,  the presi...   \n",
       "2582  in los angeles ,  the editor of a publishing h...   \n",
       "\n",
       "                                  genres  rating  \n",
       "3107                  ['Short', 'Drama']     8.0  \n",
       "900        ['Comedy', 'Crime', 'Horror']     5.6  \n",
       "6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n",
       "4704                           ['Drama']     7.4  \n",
       "2582     ['Action', 'Crime', 'Thriller']     6.6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "dataTraining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999</td>\n",
       "      <td>Message in a Bottle</td>\n",
       "      <td>who meets by fate ,  shall be sealed by fate ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1978</td>\n",
       "      <td>Midnight Express</td>\n",
       "      <td>the true story of billy hayes ,  an american c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1996</td>\n",
       "      <td>Primal Fear</td>\n",
       "      <td>martin vail left the chicago da ' s office to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1950</td>\n",
       "      <td>Crisis</td>\n",
       "      <td>husband and wife americans dr .  eugene and mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1959</td>\n",
       "      <td>The Tingler</td>\n",
       "      <td>the coroner and scientist dr .  warren chapin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                title  \\\n",
       "1  1999  Message in a Bottle   \n",
       "4  1978     Midnight Express   \n",
       "5  1996          Primal Fear   \n",
       "6  1950               Crisis   \n",
       "7  1959          The Tingler   \n",
       "\n",
       "                                                plot  \n",
       "1  who meets by fate ,  shall be sealed by fate ....  \n",
       "4  the true story of billy hayes ,  an american c...  \n",
       "5  martin vail left the chicago da ' s office to ...  \n",
       "6  husband and wife americans dr .  eugene and mr...  \n",
       "7  the coroner and scientist dr .  warren chapin ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualización datos de test\n",
    "dataTesting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer vs TFIDVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect = CountVectorizer(max_features=1000) \n",
    "#vect = TfidfVectorizer(max_features=10000) #Mejoró resultados\n",
    "\n",
    "# Definición de variables predictoras (X)\n",
    "#X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "#X_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados mejoraron usando TFIDVectorizer en lugar de CountVectorizer. Esto es porque mientras que CountVectorizer simplemente cuenta el número de veces que aparece una palabra en un texto, TF-IDF no solamente cuenta el número de veces sino que también evalúa que tan importante es la palabra en ese texto. Esto se hace gracias a la penalización de algunas palabras que el algoritmo considera menos importantes. Precisamente este factor es el que puede darle ventaja de TF-IDF sobre CountVectorizer, dado que este último pesa todas las palabras por igual, mientras que el primero ayuda a lidiar con las palabras más repetitivas y las penaliza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TFIDVectorizer + Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joseh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargamos los stopwords en una variable\n",
    "stop_words_ = list(set(stopwords.words('english')))\n",
    "\n",
    "#vect = TfidfVectorizer(max_features=10000,stop_words=stop_words_) #Quitando Stop Words Mejoró los modelos\n",
    "\n",
    "# Definición de variables predictoras (X)\n",
    "#X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "#X_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar, probamos el eliminar los stopwords o palabras más comunes usando como tokenizador el TFID vectorizer el cual nos da mejores resultados. En este caso, la combinación de la penalización de las palabras más comunes por parte del TFID y la eliminación de las palabras más repetitivas aumentó el valor predictivo del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDVectorizer + Stop words + Lematización de verbos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joseh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\joseh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Creamos un objeto WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Definición de la función que tenga como parámetro texto y devuelva una lista de lemas\n",
    "def split_into_lemmas(text):\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "## Función para lematizar los verbos en cada comentario - empeoró los resultados\n",
    "#def lemmatize_verbs(text):\n",
    "    #tagged_text = nltk.pos_tag(text.split()) # Part of speech tagging\n",
    "    #lemmatized_tokens = [lemmatizer.lemmatize(word, 'v') if tag.startswith('V') else word for word, tag in tagged_text] # Lematización solo de verbos\n",
    "    #lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    #return lemmatized_text\n",
    "\n",
    "# Vectorizamos los comentarios \n",
    "vect = TfidfVectorizer(max_features=10000,stop_words=stop_words_,analyzer=split_into_lemmas)\n",
    "\n",
    "# Definición de variables predictoras (X)\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, usamos lematización, proceso en el cual cada palabra se busca en un diccionario el cual ha sido previamente calculado y este, para cada palabra, nos va a decir cuál debe ser su representación. Usamos dos tipos de lematización, una que da la definición de la función para que tenga como parámetro texto y devuelva una lista de lemas, y el otro para lematizar los verbos en cada comentario. Los mejores valores se dieron con el primero. \n",
    "\n",
    "Por lo tanto, usamos el TFID y el Stop words dado que nos han dado los mejores valores. En conclusión, usando la mayoría de técnicas de preprocesamiento, los mejores valores los encontramoso usando FIDVectorizer + Stop words + Lematización de verbos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición de variable de interés (y)\n",
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])\n",
    "y_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=RandomForestClassifier(max_depth=10, n_jobs=-1,\n",
       "                                                     random_state=42))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definición y entrenamiento\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))\n",
    "clf.fit(X_train, y_train_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8174317146859066"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicción del modelo de clasificación\n",
    "y_pred_genres = clf.predict_proba(X_test)\n",
    "\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación variables predictoras X del conjunto de test\n",
    "X_test_dtm = vect.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "# Predicción del conjunto de test\n",
    "y_pred_test_genres = clf.predict_proba(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_Action</th>\n",
       "      <th>p_Adventure</th>\n",
       "      <th>p_Animation</th>\n",
       "      <th>p_Biography</th>\n",
       "      <th>p_Comedy</th>\n",
       "      <th>p_Crime</th>\n",
       "      <th>p_Documentary</th>\n",
       "      <th>p_Drama</th>\n",
       "      <th>p_Family</th>\n",
       "      <th>p_Fantasy</th>\n",
       "      <th>...</th>\n",
       "      <th>p_Musical</th>\n",
       "      <th>p_Mystery</th>\n",
       "      <th>p_News</th>\n",
       "      <th>p_Romance</th>\n",
       "      <th>p_Sci-Fi</th>\n",
       "      <th>p_Short</th>\n",
       "      <th>p_Sport</th>\n",
       "      <th>p_Thriller</th>\n",
       "      <th>p_War</th>\n",
       "      <th>p_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133458</td>\n",
       "      <td>0.116152</td>\n",
       "      <td>0.022739</td>\n",
       "      <td>0.045186</td>\n",
       "      <td>0.407097</td>\n",
       "      <td>0.156613</td>\n",
       "      <td>0.036416</td>\n",
       "      <td>0.508193</td>\n",
       "      <td>0.067529</td>\n",
       "      <td>0.109457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044859</td>\n",
       "      <td>0.085238</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.320989</td>\n",
       "      <td>0.065122</td>\n",
       "      <td>0.037139</td>\n",
       "      <td>0.040952</td>\n",
       "      <td>0.256173</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160392</td>\n",
       "      <td>0.102109</td>\n",
       "      <td>0.022760</td>\n",
       "      <td>0.051285</td>\n",
       "      <td>0.342970</td>\n",
       "      <td>0.228589</td>\n",
       "      <td>0.087583</td>\n",
       "      <td>0.528298</td>\n",
       "      <td>0.071921</td>\n",
       "      <td>0.081535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.164395</td>\n",
       "      <td>0.064094</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.021283</td>\n",
       "      <td>0.223114</td>\n",
       "      <td>0.032375</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.148692</td>\n",
       "      <td>0.113762</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.037688</td>\n",
       "      <td>0.278543</td>\n",
       "      <td>0.388135</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.541159</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.076762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025551</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275629</td>\n",
       "      <td>0.067862</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.373886</td>\n",
       "      <td>0.036102</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.169550</td>\n",
       "      <td>0.114147</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.035555</td>\n",
       "      <td>0.324615</td>\n",
       "      <td>0.166085</td>\n",
       "      <td>0.036024</td>\n",
       "      <td>0.576920</td>\n",
       "      <td>0.067184</td>\n",
       "      <td>0.073459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025555</td>\n",
       "      <td>0.090484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297274</td>\n",
       "      <td>0.100241</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.023298</td>\n",
       "      <td>0.287376</td>\n",
       "      <td>0.069545</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.195160</td>\n",
       "      <td>0.125724</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.249551</td>\n",
       "      <td>0.036499</td>\n",
       "      <td>0.449110</td>\n",
       "      <td>0.076839</td>\n",
       "      <td>0.082914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.170956</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.209046</td>\n",
       "      <td>0.230252</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.021034</td>\n",
       "      <td>0.269066</td>\n",
       "      <td>0.039313</td>\n",
       "      <td>0.019143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   p_Action  p_Adventure  p_Animation  p_Biography  p_Comedy   p_Crime  \\\n",
       "1  0.133458     0.116152     0.022739     0.045186  0.407097  0.156613   \n",
       "4  0.160392     0.102109     0.022760     0.051285  0.342970  0.228589   \n",
       "5  0.148692     0.113762     0.022964     0.037688  0.278543  0.388135   \n",
       "6  0.169550     0.114147     0.022964     0.035555  0.324615  0.166085   \n",
       "7  0.195160     0.125724     0.023151     0.036100  0.314815  0.249551   \n",
       "\n",
       "   p_Documentary   p_Drama  p_Family  p_Fantasy  ...  p_Musical  p_Mystery  \\\n",
       "1       0.036416  0.508193  0.067529   0.109457  ...   0.044859   0.085238   \n",
       "4       0.087583  0.528298  0.071921   0.081535  ...   0.025551   0.073773   \n",
       "5       0.021350  0.541159  0.067870   0.076762  ...   0.025551   0.179523   \n",
       "6       0.036024  0.576920  0.067184   0.073459  ...   0.025555   0.090484   \n",
       "7       0.036499  0.449110  0.076839   0.082914  ...   0.025271   0.170956   \n",
       "\n",
       "     p_News  p_Romance  p_Sci-Fi   p_Short   p_Sport  p_Thriller     p_War  \\\n",
       "1  0.000033   0.320989  0.065122  0.037139  0.040952    0.256173  0.029684   \n",
       "4  0.000896   0.164395  0.064094  0.007363  0.021283    0.223114  0.032375   \n",
       "5  0.000000   0.275629  0.067862  0.007363  0.023352    0.373886  0.036102   \n",
       "6  0.000000   0.297274  0.100241  0.007363  0.023298    0.287376  0.069545   \n",
       "7  0.000023   0.209046  0.230252  0.007363  0.021034    0.269066  0.039313   \n",
       "\n",
       "   p_Western  \n",
       "1   0.019143  \n",
       "4   0.019143  \n",
       "5   0.019143  \n",
       "6   0.019143  \n",
       "7   0.019143  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_genres_text_RF.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
